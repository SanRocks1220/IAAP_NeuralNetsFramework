# -*- coding: utf-8 -*-
"""Framework iaap.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DRiYk4Ir1uzjKSqmMf8pIKDfBEGkAMtr
"""

import numpy as np
import torch
import matplotlib.pyplot as plt
from abc import ABC, abstractmethod

class Activation:
    def __init__(self):
        pass
    
    def __call__(self, z):
        pass
    
    def diff(self, AL):
        pass

class Identity(Activation):
    def __init__(self):
        super().__init__()
    
    def __call__(self, z):
        return z
    
    def diff(self, AL):
        return np.ones_like(AL)

class Sigmoid(Activation):
    def __init__(self):
        super().__init__()
    
    def __call__(self, z):
        return 1 / (1 + np.exp(-z))
    
    def diff(self, AL):
        return np.multiply(AL, 1 - AL)

class ReLU(Activation):
    def __init__(self):
        super().__init__()

    def __call__(self, z):
        z = torch.from_numpy(z).float()
        relu = torch.nn.functional.relu(z)
        return relu.numpy()

    def diff(self, AL):
        AL = torch.from_numpy(AL).float()
        AL.requires_grad_(True)  # Habilita el cálculo de gradientes en AL
        relu_diff = torch.autograd.grad(torch.sum(torch.nn.functional.relu(AL)), AL)[0]
        return relu_diff.numpy()

class Tanh(Activation):
    def __init__(self):
        super().__init__()
    
    def __call__(self, z):
        return np.tanh(z)
    
    def diff(self, AL):
        return 1 - np.square(AL)

class Softmax(Activation):
    def __init__(self):
        pass

    def __call__(self, z):
        z = torch.from_numpy(z)
        softmax = torch.nn.functional.softmax(z, dim=1)
        return softmax.numpy()

    def diff(self, AL):
        AL = torch.from_numpy(AL)
        softmax_diff = torch.nn.functional.softmax(AL, dim=1) * (1 - torch.nn.functional.softmax(AL, dim=1))
        return softmax_diff.numpy()

class Linear(Activation):
    def __init__(self):
        super().__init__()

    def __call__(self, z):
        return z

    def diff(self, AL):
        return np.ones_like(AL)

class Loss:
    def __init__(self):
        pass
    
    def __call__(self, AL, Y):
        pass
    
    def diff(self, AL, Y):
        pass

class MeanSquaredError(Loss):
    def __init__(self):
        super().__init__()
        
    def __call__(self, AL, Y):
        AL = torch.from_numpy(AL).float()
        Y = torch.from_numpy(Y).float()
        mse_loss = torch.nn.functional.mse_loss(AL, Y)
        return mse_loss.item()

    def diff(self, AL, Y):
        AL = torch.from_numpy(AL).float()
        Y = torch.from_numpy(Y).float()
        mse_diff = 2 * (AL - Y) / AL.size(0)
        return mse_diff.numpy()

class BinaryCrossEntropy(Loss):
    def __init__(self):
        super().__init__()
        
    def __call__(self, AL, Y):
        AL = torch.from_numpy(AL).float()
        Y = torch.from_numpy(Y).float()
        bce_loss = torch.nn.functional.binary_cross_entropy(AL, Y)
        return bce_loss.item()

    def diff(self, AL, Y):
        AL = torch.from_numpy(AL).float()
        Y = torch.from_numpy(Y).float()
        bce_diff = (AL - Y) / (AL * (1 - AL))
        return bce_diff.numpy()


class CategoricalCrossEntropy(Loss):
    def __init__(self):
        super().__init__()

    def __call__(self, AL, Y):
        AL = torch.from_numpy(AL).float()
        Y = torch.from_numpy(Y).long()
        ce_loss = torch.nn.functional.cross_entropy(AL, Y)
        return ce_loss.item()

    def diff(self, AL, Y):
        AL = torch.from_numpy(AL).float()
        Y = torch.from_numpy(Y).long()
        softmax = torch.nn.functional.softmax(AL, dim=1)
        ce_diff = softmax - torch.eye(AL.shape[1])[Y]
        return ce_diff.numpy()

class Metric:
    def __init__(self):
        pass
    
    def __call__(self, y_pred, y_true):
        pass

class Accuracy(Metric):
    def __init__(self):
        super().__init__()

    def __call__(self, y_pred, y_true):
        y_pred = np.argmax(y_pred, axis=1)
        correct = np.sum(y_pred == y_true)
        total = len(y_true)
        accuracy = correct / total
        return accuracy

class Layer(ABC):

    @abstractmethod
    def forward_step(self):
        pass

    @abstractmethod
    def backward_step(self, parametro):
        pass

class Simple(Layer):
    def __init__(self):
        self.activation = Linear()
        self.weights = []
        self.bias = []
        self.Z = [] 

    def initialize_parameters(self, diment_l, diment_l1, examples, activ):
        """
        Returns:
            layers -- parameters "L1", "L2", ..., "Lm"(dictionary)
                Ll -- layer object containing proper activation, weigth and bias
        """
        self.weights = np.random.randn(diment_l1, diment_l)  * 0.01
        self.bias = np.tile(np.zeros((diment_l1,1)), (1, examples))
        self.setActivation(activ)
        self.examples = examples

    def setActivation(self, activ):
        self.activation = Linear()
        if activ == 'sigmoid':
            self.activation = Sigmoid()
        elif activ == 'relu':
            self.activation = ReLU()
        elif activ == 'tanh':
            self.activation = Tanh()
        elif activ == 'softmax':
            self.activation = Softmax()
        elif activ == 'identity':
            self.activation = Identity()

    def forward_step(self, A):
        """
        Do a forward step
        Arguments:
            A_prev -- activations from previous layer
        Returns:
            A –- activations of the actual layer
        """
        self.Z = np.dot(self.weights,A)+self.bias
        return self.activation(self.Z)

    def backward_step(self, dA, cache):
        """
        Returns:
            dA_prev – gradient of previous A (layer l-1)
            dW -- Gradient of W (current layer l)
            db -- Gradient of b (current layer l)
        """
        A = cache
        dZ = dA * self.activation.diff(self.Z)
        dW = np.dot(dZ, A.T) / self.examples
        db = np.sum(dZ, axis=1, keepdims=True) / self.examples
        dA_prev = np.dot(self.weights.T, dZ)
        return dA_prev, dW, db

class Network:
    def __init__(self, X, activ, diment, loss, metric, optim = None, regul = None):
        self.X = X
        self.activ = activ
        self.diment = diment
        self.caches = {}
        self.layers = {}
        if loss == 'categorical cross entropy':
            self.loss = CategoricalCrossEntropy()
        elif loss == 'binary cross entropy':
            self.loss = BinaryCrossEntropy()
        else:
            self.loss = MeanSquaredError()
        if metric == 'accuracy':
            self.metric = Accuracy()
        self.initialize_parameters(self.X)
                
        
    def initialize_parameters(self, X):
        """
        Returns:
            layers -- parameters "L1", "L2", ..., "Lm"(dictionary)
                Ll -- layer object containing proper activation, weigth and bias
        """
        diment = [X.shape[0]]+ self.diment
        L = len(self.diment)
        examples = X.shape[1]
        for l in range(L):
            actual_layer = Simple()
            actual_layer.initialize_parameters(diment[l],diment[l+1],X.shape[1],self.activ[l])
            self.layers['L' + str(l)] = actual_layer
        return self.layers        
            
    def forward(self, X, parameters):
        """
        Do forward propagation
        Arguments:
            X – data (array)
            layers -- parameters "L1", "L2", ..., "Lm"(dictionary)
        Returns:
            AL -- last value
            caches -- list of caches containing:
                A (there are L of them, indexed from 0 to L-1)
        """
        A = X
        self.caches['A0']=X
        cont = 1
        for l in range(len(self.layers)):
            A_prev = A
            A = parameters['L' + str(l)].forward_step(A_prev)
            self.caches['A'+ str(cont)]= A
            cont+=1
        return np.array(A), self.caches

    def predict(self, X, Y):
        """
        Predict values with a neural network
        Returns:
            AL -- last value
        """
        AL, caches = self.forward(X, self.layers)
        print("Metric =", self.metric(AL,Y))

    def train(self, X, Y, learning_rate, iterations):
        """
        Train a neural network
        Arguments:
        Returns:
            parameters -- parameters "W1", "b1", ..., "WL", "bL" (dictionary)
        """
        losses = np.array([])
        for i in range(iterations):
            AL, self.caches = self.forward(X, self.layers)
            loss = self.loss(AL, Y)
            losses = np.append(losses,loss)
            grads = self.backward(AL, Y, self.caches)
            self.layers = self.update_parameters(self.layers, grads, learning_rate)
        self.getCostPlot(losses)
        print("Loss =", losses[-1])

    def getCostPlot(self, losses):
        fig1, ax1 = plt.subplots()
        ax1.plot(np.arange(len(losses)), losses, label='Loss')
        ax1.set_xlabel('epocas')
        ax1.set_ylabel('costo')
        ax1.legend()
        plt.show(block=False)    

    def backward(self, AL, Y, caches):
        """
        Returns:
            grads -- gradients "dW1", "db1", ..., "dWL", "dbL" (dictionary)
        """
        grads = {}
        L = len(caches) - 1
        m = AL.shape[1]
        dAL = self.loss.diff(AL, Y)
        dA = dAL
        for l in reversed(range(L)):            
            dA_prev, dW, db = self.layers["L" + str(l)].backward_step(dA, self.caches["A" + str(l)])
            grads["dA" + str(l)] = dA_prev
            grads["dW" + str(l)] = dW
            grads["db" + str(l)] = db
            dA = dA_prev
        return grads
    
    def update_parameters(self, parameters, grads, learning_rate):
        """
        Returns:
            parameters -- parameters "W1", "b1", ..., "WL", "bL" (dictionary)
        """
        L = len(parameters)
        for l in range(L):
            parameters["L" + str(l)].weights -= learning_rate * grads["dW" + str(l)]
            parameters["L" + str(l)].bias -= learning_rate * grads["db" + str(l)]
        return parameters
